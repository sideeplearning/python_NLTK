{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Si\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Si\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from os import path\n",
    "import re\n",
    "import libs as ft\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from pyfasttext import FastText\n",
    "#from gensim.models.wrappers import FastText\n",
    "import fasttext as ft\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (roc_curve, auc, accuracy_score)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(x, drop_tag, tag_pos, lemmatizer):\n",
    "    \"\"\"\n",
    "    いらない品詞を除外し，レンマ化して返す．apply関数内で使用，\n",
    "\n",
    "    Args:\n",
    "        x (Series): apply関数で呼び出されるSeries\n",
    "        drop_tag (list): いらない品詞リスト(nltk)\n",
    "        tag_pos (dict): key -> tag, value -> pos. レンマ化の精度向上に使用．\n",
    "        lemmatizer (nltk.stem.WordNetLemmatizer): lemmatizer\n",
    "\n",
    "    Returns:\n",
    "        (str): output sentence\n",
    "    \"\"\"\n",
    "    words = [word for word in x['headline_text'].split(' ') if word != '']  # 空文字入るとエラーになる\n",
    "    tags = nltk.pos_tag(words)  # 品詞を取得\n",
    "    words = [(word, tag_pos[tag]) for word, tag in tags if tag not in drop_tag]  # いらない品詞を除外\n",
    "    print('words:',words)\n",
    "    words = [lemmatizer.lemmatize(word, pos=pos) for word, pos in words]\n",
    "    sentence = ' '.join(words)  # 連結\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    \"\"\"\n",
    "    前処理の関数．\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): input dataset\n",
    "\n",
    "    Retruns:\n",
    "        (DataFrame): output dataset\n",
    "    \"\"\"\n",
    "    # まずは，いらない品詞を落とし，レンマ化する．\n",
    "    # その後，階層クラスタリングのときに使う用のcsvファイルとモデル学習用のtxtファイルを出力する．\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    # いらない品詞\n",
    "    drop_tag = ['$', 'CC', 'CD', 'DT', 'IN', 'MD', 'POS', 'PRP', 'PRP$', 'RP', 'TO' , 'WP', 'WRB','WDT','PDT']\n",
    "    # 品詞とpos(lemma用)の変換辞書\n",
    "    tag_pos = {'FW': 'n', 'JJ': 'a', 'JJR': 'a', 'JJS': 'a', 'NN': 'n', 'NNP': 'n', 'NNS': 'n', 'RB': 'r', 'RBR': 'r', 'VB': 'v',\n",
    "               'VBD': 'v', 'VBG': 'v', 'VBN': 'v', 'VBP': 'v', 'VBZ': 'v', 'RBS': 'r',}\n",
    "\n",
    "    #data = data.assign(preprocessed=data.apply(func=cleansing, axis=1, args=(drop_tag, tag_pos, lemmatizer,)))\n",
    "    data = data.assign(preprocessed=data.apply(func=cleansing, axis=1, args=(drop_tag, tag_pos, lemmatizer)))\n",
    "\n",
    "    print('after drop and lemmatization')\n",
    "    print(data.head())\n",
    "    data.to_csv('data.csv', sep='\\t', index=False)\n",
    "    #data['preprocessed'].to_csv('text.txt', index=False)\n",
    "    data['preprocessed'].to_csv('text.txt', index=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(data_name='text.txt', model_name='./pretrained_model/model.bin'):\n",
    "    \"\"\"\n",
    "    fasttextベースで分散表現を取得する関数．これも見てわかると思うので引数は省略．\n",
    "\n",
    "    Returns:\n",
    "        (list of list): 単語リストのリスト．[['word_0_0', 'word_0_1'], ['word_1_0', 'word_1_1', 'word_1_2'], ...]みたいな\n",
    "        (array): 分散表現 次元=(文章数×分散表現の次元数)\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    with open(data_name, mode='r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = re.sub('\\n', '', line)\n",
    "            sentences.append(line.split(' '))\n",
    "\n",
    "    # modelが12GBくらいメモリを食うので終わったら開放する．\n",
    "    vec_name =  'sentences_vc.npy'\n",
    "    #if not path.exists(vec_name):\n",
    "        #model = FastText.load_fasttext_format(model_name)\n",
    "    model = ft.load_model(model_name)\n",
    "    dim = model.get_dimension()\n",
    "    sentences_vec = np.zeros((dim,))\n",
    "\n",
    "    for words in sentences:\n",
    "        vec = np.zeros((dim,))\n",
    "        for word in words:\n",
    "            if model.get_word_id(word) == -1:\n",
    "                print('this word does not exists in corpus: %s at %s' % (word, words))\n",
    "            vec = np.vstack((vec, model.get_word_vector(word)))\n",
    "        vec = vec[1:, :].mean(axis=0)\n",
    "        sentences_vec = np.vstack((sentences_vec, vec))\n",
    "    sentences_vec = sentences_vec[1:, :]\n",
    "    del model\n",
    "\n",
    "    np.save(vec_name, sentences_vec)\n",
    "    #else:\n",
    "        #sentences_vec = np.load(vec_name)\n",
    "    return sentences, sentences_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from contextlib import redirect_stdout\n",
    "from io import StringIO\n",
    "\n",
    "example = 'Mary had a little lamb, Jack went up the hill, Jill followed suit, i woke up suddenly, it was a really bad dream...'\n",
    "\n",
    "def token_to_sentence(str):\n",
    "    f = StringIO()\n",
    "    with redirect_stdout(f):\n",
    "        regex_of_sentence = re.findall('([\\w\\s]{0,})[^\\w\\s]', str)\n",
    "        regex_of_sentence = [x for x in regex_of_sentence if x is not '']\n",
    "        for i in regex_of_sentence:\n",
    "            print(i)\n",
    "        first_step_to_sentence = (f.getvalue()).split('\\n')\n",
    "    g = StringIO()\n",
    "    with redirect_stdout(g):\n",
    "        for i in first_step_to_sentence:\n",
    "            try:\n",
    "                regex_to_clear_sentence = re.search('\\s([\\w\\s]{0,})', i)\n",
    "                print(regex_to_clear_sentence.group(1))\n",
    "            except:\n",
    "                print(i)\n",
    "        sentence = (g.getvalue()).split('\\n')\n",
    "    return sentence\n",
    "\n",
    "def token_to_words(str):\n",
    "    f = StringIO()\n",
    "    with redirect_stdout(f):\n",
    "        for i in str:\n",
    "            regex_of_word = re.findall('([\\w]{0,})', i)\n",
    "            regex_of_word = [x for x in regex_of_word if x is not '']\n",
    "            for word in regex_of_word:\n",
    "                print(regex_of_word)\n",
    "        words = (f.getvalue()).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Si\\Desktop\\python_all\\python_NLTK\\NLTK_/Poe-story\n"
     ]
    }
   ],
   "source": [
    "#homePC\n",
    "#path_0 = Path(r'C:\\Users\\SI\\Python_ML\\python_NLTK\\NLTK_')\n",
    "#officePC\n",
    "path_0 = Path(r'C:\\Users\\Si\\Desktop\\python_all\\python_NLTK\\NLTK_')\n",
    "\n",
    "\n",
    "author = ['Poe','Twain']\n",
    "type_ = ['letter', 'story']\n",
    "test = ['test']\n",
    "\n",
    "choice = [0,1]\n",
    "\n",
    "index_i = 0\n",
    "index_j = 1\n",
    "path_file = str(path_0) + str('/') + str(author[choice[index_i]]) + str('-') + str(type_[choice[index_j]])\n",
    "#path = str(path_0) + str('/') + str(test[0])\n",
    "print(path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lists: <generator object Path.glob at 0x0000023AA92876C8>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-001.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-002.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-003.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-004.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-005.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-006.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-007.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-008.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-009.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-010.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-011.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-012.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-013.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-014.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-015.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-016.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-017.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-018.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-019.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-020.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-021.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-022.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-023.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-024.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-025.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-026.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-027.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-028.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-029.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-030.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-031.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-032.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-033.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-034.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-035.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-036.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-037.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-038.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-039.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-040.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-041.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-042.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-043.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-044.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-045.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-046.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-047.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-048.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-049.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-050.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-051.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-052.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-053.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-054.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-055.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-056.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-057.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-058.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-059.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-060.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-061.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-062.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-063.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-064.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-065.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-066.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-067.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-068.txt'),\n",
       " WindowsPath('C:/Users/Si/Desktop/python_all/python_NLTK/NLTK_/Poe-story/Poe-story-069.txt')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_file = Path(path_file)\n",
    "path_file\n",
    "path_file.glob('*.txt')\n",
    "list(path_file.glob('*.txt'))\n",
    "print('lists:', path_file.glob('*.txt'))\n",
    "#list path\n",
    "path_list = list(path_file.glob('*.txt'))\n",
    "path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>THE SWISS BELL-RINGERS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the regular allies of the Mirror, a man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We have argued the point with him till we are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>While these documents are coming, we publish t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The readers of the Mirror scarce need be told,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The writer alludes to them now only to say, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>For this reason, too, they arrange so carefull...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Their very number shows that they were contriv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              THE SWISS BELL-RINGERS\n",
       "0  One of the regular allies of the Mirror, a man...\n",
       "1  We have argued the point with him till we are ...\n",
       "2  While these documents are coming, we publish t...\n",
       "3  The readers of the Mirror scarce need be told,...\n",
       "4  The writer alludes to them now only to say, th...\n",
       "5  For this reason, too, they arrange so carefull...\n",
       "6  Their very number shows that they were contriv..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_table(path_list[0], encoding='cp932')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,\n",
       " ['One of the regular allies of the Mirror, a man of a very humorous critical vein, has taken it into his head to prove the Swiss Bell-ringers to be an automaton. ',\n",
       "  'We have argued the point with him till we are tired, and have at last sent to beg a copy of their board-bill with affidavits that their stomachs are not wooden and do kindly entertain rolls and sausages.',\n",
       "  \"While these documents are coming, we publish the skeleton of our friend's hypothesis: The Swiss Bell-ringers. \",\n",
       "  'The readers of the Mirror scarce need be told,  as most of them have seen and heard for themselves,  that the Swiss Bell-ringers enter, to the number of seven, white-plumed and fancifully costumed, and each armed with four or five hand-bells of various sizes, which they deposit on a cushioned table before them, retaining one in each hand, which they are continually changing for others in their armory, putting down and taking up with the rapidity of jugglers, and all the while ringing the changes upon them with a delicate harmony and precision, which are as perfect in a symphony of Haydn as in Miss Lucy Long. ',\n",
       "  \"The writer alludes to them now only to say, that they may be heard again to-night, and to correct the erroneous but common idea that these Bell-ringers are real living beings. The writer is firmly convinced that they are ingenious pieces of mechanism, contrived on the principle of Maelzel's Automaton Trumpeter and Piano-forte player (exhibited here some years ago), but made so much more perfect and effective by the application to them of the same power which operates in the Electro-Magnetic Telegraph,but which should here be called Electro-tintinnabulic. A powerful electric battery under the stage communicates by a hidden wire with each of them, and its shocks are regulated and directed by the skilful musician and mechanician who secretly man[a]ges the whole affair. This explains the precision with which they all bow at the same instant, as if moved by the same soul (and so they are an electric one), and keep such perfect time and order.\",\n",
       "  \"For this reason, too, they arrange so carefully their surplus bells before them in such exact spots, just as Maelzel's Automaton Chess-player always insisted on the pieces being placed exactly on the centre of the squares, so that his mechanically-moved fingers might not miss them. \",\n",
       "  'Their very number shows that they were contrived in imitation of the music of the seven spheres,and any lurking doubt of the truth of our theory will be at once removed by noticing how they electrify their hearers.'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = data[data.columns[0]].values.tolist()\n",
    "len(data_list),data_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# divided into short sentences\n",
    "sentence = token_to_sentence(data_list[1])\n",
    "print(sentence)\n",
    "\n",
    "df_sentence = pd.DataFrame(sentence)\n",
    "df_sentence = df_sentence.rename(columns={0: 'headline_text'})\n",
    "df_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have argued the point with him till we are tired, and have at last sent to beg a copy of their board-bill with affidavits that their stomachs are not wooden and do kindly entertain rolls and sausages. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We have argued the point with him till we are ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  We have argued the point with him till we are ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# divided into long sentences\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "\n",
    "para = data_list[1]\n",
    "\n",
    "punkt_params = PunktParameters()\n",
    "punkt_params.abbrev_types = set(['Mr', 'Mrs', 'LLC','Miss'])\n",
    "tokenizer = PunktSentenceTokenizer(punkt_params)\n",
    "tokens = tokenizer.tokenize(para)\n",
    "\n",
    "for t in tokens:\n",
    "    print (t, \"\\n\")\n",
    "    \n",
    "df_sentence = pd.DataFrame(tokens)\n",
    "df_sentence = df_sentence.rename(columns={0: 'headline_text'})\n",
    "df_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We have argued the point with him till we are ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  We have argued the point with him till we are ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentence = df_sentence[:]\n",
    "df_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'headline_text'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = pd.DataFrame(df_sentence).columns[0]\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    We have argued the point with him till we are ...\n",
       " Name: headline_text, dtype: object,\n",
       " pandas.core.series.Series,\n",
       " (1,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = pd.DataFrame(df_sentence)['headline_text']\n",
    "series, type(series), series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: [('have', 'v'), ('argued', 'v'), ('point', 'n'), ('are', 'v'), ('tired,', 'a'), ('have', 'v'), ('last', 'a'), ('sent', 'n'), ('beg', 'v'), ('copy', 'n'), ('board-bill', 'n'), ('affidavits', 'n'), ('stomachs', 'n'), ('are', 'v'), ('not', 'r'), ('wooden', 'a'), ('do', 'v'), ('kindly', 'r'), ('entertain', 'v'), ('rolls', 'n'), ('sausages.', 'n')]\n",
      "after drop and lemmatization\n",
      "                                       headline_text  \\\n",
      "0  We have argued the point with him till we are ...   \n",
      "\n",
      "                                        preprocessed  \n",
      "0  have argue point be tired, have last sent beg ...  \n",
      "this word does not exists in corpus: \"have at ['\"have', 'argue', 'point', 'be', 'tired,', 'have', 'last', 'sent', 'beg', 'copy', 'board-bill', 'affidavit', 'stomach', 'be', 'not', 'wooden', 'do', 'kindly', 'entertain', 'roll', 'sausages.\"']\n",
      "this word does not exists in corpus: tired, at ['\"have', 'argue', 'point', 'be', 'tired,', 'have', 'last', 'sent', 'beg', 'copy', 'board-bill', 'affidavit', 'stomach', 'be', 'not', 'wooden', 'do', 'kindly', 'entertain', 'roll', 'sausages.\"']\n",
      "this word does not exists in corpus: board-bill at ['\"have', 'argue', 'point', 'be', 'tired,', 'have', 'last', 'sent', 'beg', 'copy', 'board-bill', 'affidavit', 'stomach', 'be', 'not', 'wooden', 'do', 'kindly', 'entertain', 'roll', 'sausages.\"']\n",
      "this word does not exists in corpus: sausages.\" at ['\"have', 'argue', 'point', 'be', 'tired,', 'have', 'last', 'sent', 'beg', 'copy', 'board-bill', 'affidavit', 'stomach', 'be', 'not', 'wooden', 'do', 'kindly', 'entertain', 'roll', 'sausages.\"']\n",
      "(2, 300) [['preprocessed'], ['\"have', 'argue', 'point', 'be', 'tired,', 'have', 'last', 'sent', 'beg', 'copy', 'board-bill', 'affidavit', 'stomach', 'be', 'not', 'wooden', 'do', 'kindly', 'entertain', 'roll', 'sausages.\"']]\n"
     ]
    }
   ],
   "source": [
    "df_sentence = preprocess(pd.DataFrame(series))\n",
    "sentences, vec = get_word_vector()\n",
    "print(vec.shape, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 300) [[-2.51920950e-02  1.00359626e-01 -2.20984653e-01  2.16720119e-01\n",
      "  -3.75728428e-01 -1.36016786e-01  4.69669819e-01 -3.95488858e-01\n",
      "   2.93971002e-01 -3.41169566e-01 -1.44786894e-01  7.74884075e-02\n",
      "  -1.14528291e-01 -9.62964892e-02  2.69352525e-01 -5.01342714e-01\n",
      "   1.66222557e-01  2.17696428e-01  1.19996667e-01  2.15265140e-01\n",
      "   2.96215922e-01 -2.80547261e-01 -1.84577957e-01  1.52415022e-01\n",
      "  -3.04059654e-01  1.11938834e-01  9.00849104e-02  6.92548826e-02\n",
      "  -9.44551229e-02  1.00656733e-01  1.44979969e-01  5.78513265e-01\n",
      "   2.69297175e-02  2.76256263e-01  5.39283529e-02 -7.12620094e-02\n",
      "  -7.91453570e-02  3.90956819e-01  1.14514515e-01 -7.77416676e-02\n",
      "   9.29599535e-03 -3.71775478e-01  7.99704939e-02 -3.70187163e-02\n",
      "   9.73515736e-04 -3.15918356e-01  4.33003038e-01 -3.00064474e-01\n",
      "  -4.21907753e-02  2.79742539e-01  1.89448118e-01 -5.66520572e-01\n",
      "  -2.27706507e-01  7.20087960e-02  2.75513172e-01 -1.88710138e-01\n",
      "   9.33695957e-02 -2.30479017e-01 -2.49538198e-01  1.21192001e-01\n",
      "  -5.21889329e-01  2.29512349e-01 -3.73127684e-02  6.31032558e-03\n",
      "   2.47290418e-01 -2.50866473e-01 -1.50823191e-01  8.27862248e-02\n",
      "  -1.80721000e-01  5.33149600e-01  9.82950330e-02  4.52998579e-01\n",
      "   5.06900847e-01 -1.35424078e-01  2.72423208e-01  6.39922142e-01\n",
      "   6.74659014e-02 -4.07341480e-01 -8.25086161e-02 -7.56775960e-02\n",
      "   2.85322100e-01  2.26164281e-01 -3.70744020e-01  1.03522681e-01\n",
      "  -1.04356430e-01 -2.36097053e-01  4.46460724e-01  6.87342361e-02\n",
      "   3.48580368e-02 -4.21798266e-02  7.26212114e-02  2.35584214e-01\n",
      "   3.52002829e-01 -1.29660284e-02 -1.18488930e-01 -1.05749108e-02\n",
      "  -1.94726303e-01  3.02888483e-01 -1.12108611e-01  1.74106285e-01\n",
      "   3.32015663e-01 -6.29690289e-01  5.76910257e-01 -1.26580924e-01\n",
      "  -2.32360717e-02 -4.96254340e-02 -1.80268735e-01 -6.19499944e-02\n",
      "  -6.77938759e-02  3.36486161e-01 -2.03460619e-01  1.18595101e-01\n",
      "  -5.67825198e-01  9.08640679e-03 -3.83060090e-02  1.34620056e-01\n",
      "   2.79577553e-01 -1.61369946e-02  2.96554804e-01 -2.06327498e-01\n",
      "   3.55299264e-01  6.79842830e-02 -1.38461769e-01  8.44042480e-01\n",
      "   1.98829874e-01 -9.19638425e-02  6.83374256e-02 -3.53507549e-02\n",
      "   1.83749929e-01  3.25242668e-01  2.67845631e-01  8.25800747e-02\n",
      "  -1.73350632e-01  1.02009758e-01  2.84689695e-01  1.12998687e-01\n",
      "  -1.43617198e-01 -2.03525990e-01  6.79836869e-02  3.24487507e-01\n",
      "   7.10131824e-01 -1.47976428e-01  3.03853333e-01 -1.94384471e-01\n",
      "  -1.92151740e-01  9.82049480e-02  3.61585338e-03 -2.54584968e-01\n",
      "  -1.47868603e-01  4.23649251e-02  1.40202388e-01 -1.09883420e-01\n",
      "   1.44664764e-01 -6.90097690e-01 -3.08403522e-01 -2.84220189e-01\n",
      "   5.68355843e-02 -2.64808605e-03 -7.35538006e-02  4.13892031e-01\n",
      "   2.39737146e-02 -5.46984188e-03 -5.70144892e-01  1.55444145e-01\n",
      "   9.20065194e-02  3.53992194e-01 -1.89053953e-01 -4.81645405e-01\n",
      "   3.18490975e-02  2.43839726e-01 -8.74094069e-01 -6.81947291e-01\n",
      "   4.77093190e-01  3.64016086e-01 -3.56081575e-01  1.85342655e-01\n",
      "  -3.92920732e-01 -6.07440695e-02 -3.46878618e-01  8.26023892e-03\n",
      "   1.20160803e-01  9.99623910e-03  1.14376405e-02 -1.80554882e-01\n",
      "   1.71071246e-01  3.43532503e-01  3.17832604e-02  3.76558714e-02\n",
      "   4.36131239e-01  4.77046162e-01  3.68017852e-01 -1.41571492e-01\n",
      "  -3.92020792e-01  6.87558157e-03 -4.49125648e-01 -3.09014022e-01\n",
      "   1.10540032e-01  1.21972868e-02 -6.13967359e-01  2.01035425e-01\n",
      "   6.00172915e-02 -1.16395548e-01  1.36069074e-01  5.65699972e-02\n",
      "   2.64258206e-01 -3.26190889e-01  1.30278826e-01  4.41227049e-01\n",
      "   3.36103261e-01 -1.20026641e-01  4.62475002e-01  1.97294191e-01\n",
      "  -5.38299754e-02  2.87822455e-01 -1.01951309e-01 -4.66920957e-02\n",
      "  -6.46838844e-02 -8.85580480e-01 -2.67359763e-01  8.25021088e-01\n",
      "   2.25115627e-01  1.16818726e-01  3.53605777e-01  2.47993797e-01\n",
      "  -3.03442334e-03 -4.07129377e-02 -5.27364612e-01 -6.01245105e-01\n",
      "  -2.22509935e-01 -2.32056677e-01 -6.23121392e-03 -3.77529413e-01\n",
      "  -6.16464913e-01 -1.01907961e-01 -3.85952026e-01 -1.30085319e-01\n",
      "  -1.91825226e-01 -1.21286377e-01  2.52656370e-01  1.00415006e-01\n",
      "   1.87328830e-01  2.49531597e-01  5.27173877e-01 -2.79237181e-01\n",
      "  -5.37515223e-01  1.52175531e-01 -1.62327603e-01  1.41135290e-01\n",
      "   1.10678904e-01  1.32514521e-01 -1.90034628e-01 -2.39711776e-01\n",
      "  -4.48460609e-01  1.22372307e-01  5.79947293e-01  3.67036760e-01\n",
      "   6.08926356e-01  2.33818695e-01 -1.10818706e-01  1.94630980e-01\n",
      "   4.55078222e-02  1.16734080e-01 -3.84466276e-02  2.08607823e-01\n",
      "  -2.23470017e-01 -2.64023930e-01 -1.22706011e-01 -6.38743758e-01\n",
      "  -1.66267395e-01 -1.42601520e-01 -2.18540773e-01  1.73913002e-01\n",
      "   2.18134969e-01 -1.73364609e-01 -4.44188751e-02  1.28485812e-02\n",
      "   2.72878826e-01 -1.00144595e-02  2.51280725e-01  9.37756244e-03\n",
      "   1.43394142e-01  1.35677889e-01 -3.50547701e-01 -2.65410483e-01\n",
      "  -1.98949397e-01 -2.14630887e-01  2.64833808e-01 -3.27212453e-01\n",
      "  -4.09795046e-01  2.17806906e-01  1.04765981e-01 -8.96625742e-02\n",
      "  -2.46274009e-01  2.06871167e-01  5.49404860e-01 -1.71527669e-01\n",
      "  -1.51982889e-01  1.37992680e-01  2.23820508e-01  6.53910935e-02]\n",
      " [-1.41831754e-01  3.83370491e-02 -8.96858910e-02  1.48892499e-01\n",
      "  -8.90158314e-02 -4.02380609e-02 -3.39847496e-02 -1.78267500e-01\n",
      "  -3.70657891e-02  7.64667588e-02 -2.78450563e-02 -2.73397574e-02\n",
      "  -1.17423388e-01 -3.05426691e-02  2.65896970e-02 -1.69622320e-01\n",
      "  -3.43681413e-02  1.47888443e-01  1.69423168e-02  1.06277269e-01\n",
      "  -3.51541549e-02  1.04710653e-01 -1.34889108e-01 -9.59285422e-02\n",
      "  -8.46445281e-02 -3.76034395e-02 -1.18832257e-01 -1.98587226e-02\n",
      "   2.14669920e-02  1.16616901e-01 -5.86608267e-02  6.88040454e-02\n",
      "  -2.33131015e-01  4.67906598e-02 -5.62080452e-02 -7.28203873e-02\n",
      "  -2.77327262e-02 -1.88010784e-02  6.03388713e-02 -1.10553274e-01\n",
      "   1.25383462e-01 -1.67787849e-02 -5.25881864e-02  6.69575661e-03\n",
      "   5.59389825e-02  8.65877125e-02  1.29843438e-01 -7.56655622e-02\n",
      "  -5.88850948e-03  7.62642226e-03  3.28030462e-02 -2.40157880e-01\n",
      "  -1.12239607e-02 -2.40886224e-02 -1.04571989e-01  7.41329357e-02\n",
      "   2.26145912e-02  5.73130360e-02 -1.44187597e-01  1.84726604e-01\n",
      "  -3.78200637e-02 -1.05744925e-01  8.99675572e-02 -3.07348832e-03\n",
      "   4.80897729e-03 -1.33948997e-01 -1.18535524e-01  1.29819814e-01\n",
      "   2.57500470e-02 -5.25894628e-02  6.34633659e-02  3.81527019e-02\n",
      "   1.46317284e-01 -2.07059341e-01 -5.82508842e-02  1.39785225e-01\n",
      "   2.17636564e-01  1.87350666e-01 -5.63738967e-02 -3.84750725e-02\n",
      "   9.38990014e-02  2.53419844e-02  1.55219663e-02 -1.08023091e-02\n",
      "  -7.99494090e-02  1.83500021e-02 -2.00398076e-02  3.40091173e-02\n",
      "  -5.55050153e-02 -7.09461002e-02  3.27327280e-02 -5.51694889e-02\n",
      "   1.79967942e-01 -2.27472580e-01  1.33401160e-01  1.81198204e-01\n",
      "   4.66184284e-02 -5.65992064e-02 -8.14551249e-02  3.89645325e-02\n",
      "  -1.73145265e-02 -9.70507112e-02 -8.75599829e-03 -3.07790492e-02\n",
      "   4.54846523e-03 -9.31634799e-02 -1.27666861e-01  3.42200342e-02\n",
      "  -7.64270757e-03  6.18851267e-02  8.09844246e-03 -5.73013702e-02\n",
      "  -5.43038872e-02 -2.81554614e-02  8.20586839e-03 -1.02448043e-01\n",
      "   1.92337811e-01  3.24618482e-02 -9.34231898e-02  1.05231344e-01\n",
      "   1.31602787e-01  8.46038927e-02 -3.27822669e-02  3.21456444e-01\n",
      "   2.81028383e-02 -3.15439908e-02  2.24858855e-04  2.27686966e-02\n",
      "  -2.64785302e-02  3.13847467e-01 -6.70401622e-02  6.97112113e-02\n",
      "  -1.51146553e-01  1.23145299e-01  1.23583753e-01 -1.64987168e-02\n",
      "  -4.24776885e-02 -6.00186643e-02 -1.99415939e-02  8.48907005e-02\n",
      "  -6.83481863e-02  9.23096932e-02  1.30212222e-01 -5.77644591e-02\n",
      "  -1.79463415e-02  1.60777532e-01 -7.97665063e-02 -1.29637337e-01\n",
      "  -2.36134946e-02  3.14825740e-02  1.50296073e-01 -1.84763237e-01\n",
      "  -1.54273777e-02  3.55850732e-02 -1.45965650e-01  1.74398913e-02\n",
      "  -1.23521974e-01 -4.74239492e-02  1.81559342e-01  1.11295394e-02\n",
      "  -2.30023951e-02  1.66087307e-01 -1.87235391e-01  1.34886549e-02\n",
      "   8.09531504e-03  1.21467502e-01 -3.29185694e-02 -1.63782304e-01\n",
      "   2.78135498e-01  6.20970941e-02 -2.23032731e-01 -2.00376072e-01\n",
      "  -2.04176139e-01  1.89981837e-02 -1.01101415e-01  2.42678594e-01\n",
      "  -6.92299865e-02  1.41210182e-01  5.89572847e-02 -1.61392036e-01\n",
      "   1.37385247e-03 -2.04922115e-01  9.33698139e-02  7.77284758e-02\n",
      "  -1.06889710e-01  1.87813618e-01 -6.53169875e-02 -1.37284178e-01\n",
      "   2.18496534e-01 -4.30530363e-02  4.00817066e-02 -2.09335229e-01\n",
      "   2.44500240e-02  1.64035137e-01 -1.03454969e-01 -1.39668622e-01\n",
      "  -9.27855827e-02  1.42670853e-02 -3.09963103e-01  1.34270745e-02\n",
      "   2.87819729e-02 -5.05770346e-02  1.03733932e-01 -7.67845257e-03\n",
      "  -7.30272041e-02 -1.00394307e-01 -9.64309688e-02  3.79599277e-02\n",
      "   1.71530161e-01  3.59320040e-02  3.42528294e-01  1.40841546e-02\n",
      "   1.06950139e-01 -1.20214165e-01  1.19859482e-01 -3.86738657e-02\n",
      "   1.96691522e-02 -1.84641270e-01  1.99215562e-02  1.10208526e-01\n",
      "  -8.48360898e-02  1.58948537e-02 -7.16723354e-02 -4.63494833e-02\n",
      "  -7.68334731e-03  1.08909599e-01 -1.40357389e-01 -6.50704696e-02\n",
      "   4.58682485e-02 -9.16986345e-03  1.10447829e-01  5.73598123e-03\n",
      "  -4.07043229e-02  3.63284074e-02  1.26777120e-02  5.37033702e-02\n",
      "   5.45643098e-02 -1.53267002e-02  5.27495728e-02 -4.95987091e-02\n",
      "  -4.09298255e-02  8.76422790e-02  1.29163193e-01  6.07764198e-02\n",
      "  -1.38867062e-01  1.47314876e-01  1.30383268e-01  5.14307705e-02\n",
      "  -3.17555018e-03 -2.26330156e-02  1.30286292e-01 -6.49156195e-02\n",
      "  -1.62856821e-01  3.42112804e-02  6.50232262e-02  6.93375607e-02\n",
      "   3.85248844e-02 -2.32153694e-01  1.63407095e-02 -9.63388500e-02\n",
      "  -2.25031663e-02  1.23509425e-01 -5.33784279e-02 -3.57522560e-02\n",
      "  -2.25238388e-01 -1.10827929e-01 -5.81273858e-02  1.71818210e-01\n",
      "  -9.01352800e-03 -6.49002239e-03 -1.22997370e-01  2.21672486e-02\n",
      "   7.05249766e-02  6.62097440e-02 -1.13024649e-01 -1.60844124e-01\n",
      "   1.41042155e-01  1.71718252e-01 -4.81578042e-02 -7.99598625e-02\n",
      "   8.25021620e-02  5.97504084e-02 -2.98060926e-02 -3.76483303e-02\n",
      "   1.13942350e-01 -2.69145286e-03  7.19721037e-02  1.63711610e-01\n",
      "   5.21853689e-02  2.96481794e-02  6.68496872e-02  2.86074568e-03\n",
      "  -4.95976035e-02 -2.56116009e-02  2.96692247e-02 -6.92136501e-02\n",
      "  -1.45104272e-01  1.81791422e-01  7.34325024e-02  6.69831547e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(vec.shape, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function exists at 0x0000023A9EDDD678>\n"
     ]
    }
   ],
   "source": [
    "print(path.exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Either the well was very deep, or she fell very slowly, for she had plenty\n",
      "of time as she went down to look about her and to wonder what was going to happen\n",
      "next. \n",
      "\n",
      "First, she tried to look down and make out what she was coming to, but it was\n",
      "too dark to see anything; then she looked at the sides of the well, and noticed\n",
      "that they were filled with cupboards and book-shelves; here and there she saw maps\n",
      "and pictures Mr...hung upon pegs. \n",
      "\n",
      "She took down a jar from one of the shelves as she\n",
      "passed; it was labelled 'ORANGE MARMALADE', but to her great disappointment it was\n",
      "empty: she did not like to drop the jar for fear of killing somebody, so managed to\n",
      "put it into one of the cupboards as she fell past it. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "\n",
    "para = '''Either the well was very deep, or she fell very slowly, for she had plenty\n",
    "of time as she went down to look about her and to wonder what was going to happen\n",
    "next. First, she tried to look down and make out what she was coming to, but it was\n",
    "too dark to see anything; then she looked at the sides of the well, and noticed\n",
    "that they were filled with cupboards and book-shelves; here and there she saw maps\n",
    "and pictures Mr...hung upon pegs. She took down a jar from one of the shelves as she\n",
    "passed; it was labelled 'ORANGE MARMALADE', but to her great disappointment it was\n",
    "empty: she did not like to drop the jar for fear of killing somebody, so managed to\n",
    "put it into one of the cupboards as she fell past it.'''\n",
    "\n",
    "punkt_params = PunktParameters()\n",
    "#punkt_params.abbrev_types = set(['Mr', 'Mrs', 'LLC'])\n",
    "tokenizer = PunktSentenceTokenizer(punkt_params)\n",
    "tokens = tokenizer.tokenize(para)\n",
    "\n",
    "for t in tokens:\n",
    "    print (t, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
